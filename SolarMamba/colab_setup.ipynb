{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e52e675",
   "metadata": {},
   "source": [
    "# SolarMamba — Google Colab Setup\n",
    "\n",
    "**Spatiotemporal Solar Irradiance Forecasting with MambaVision + PyramidTCN**\n",
    "\n",
    "### Prerequisites — before running this notebook:\n",
    "1. Set Colab runtime to **GPU** (`Runtime → Change runtime type → T4 / A100`)\n",
    "2. Upload your dataset to **Google Drive** (Option A) or a **HuggingFace Dataset repo** (Option B)\n",
    "3. Push your code to a **GitHub repository** (public or private)\n",
    "4. Make sure `weights/mambavision_b_1k.pth` is accessible (Drive or GitHub LFS)\n",
    "\n",
    "---\n",
    "### Execution order:\n",
    "Run the cells **top to bottom in order** — each section depends on the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75eb5b",
   "metadata": {},
   "source": [
    "## Step 1 — Verify GPU Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da3a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "gpu_info = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "if gpu_info.returncode != 0:\n",
    "    print(\"⚠️  NO GPU DETECTED.\")\n",
    "    print(\"    Go to Runtime → Change runtime type → Hardware accelerator → GPU (T4 or A100).\")\n",
    "    print(\"    Then re-run this notebook from the top.\")\n",
    "else:\n",
    "    print(gpu_info.stdout.split('\\n')[0])   # Print first line (driver / CUDA version)\n",
    "    print(gpu_info.stdout.split('\\n')[8])   # Print the GPU name line\n",
    "    print(\"\\n✅ GPU is available. Proceeding with installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d8fca4",
   "metadata": {},
   "source": [
    "## Step 2 — Install PyTorch (Must Match Colab's CUDA)\n",
    "\n",
    "> **Why pin versions?**  `mamba-ssm` and `causal-conv1d` build CUDA extensions at install time.  \n",
    "> They **must** be compiled against the exact same PyTorch + CUDA version that will be used at runtime.  \n",
    "> Colab currently ships CUDA 12.1 runtimes, so we pin `torch==2.4.0+cu121`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚡ Installs torch 2.4.0 built against CUDA 12.1 — matches Colab's default GPU driver.\n",
    "# If Colab upgrades its driver in the future, check: https://pytorch.org/get-started/previous-versions/\n",
    "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 \\\n",
    "    --index-url https://download.pytorch.org/whl/cu121 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6978f911",
   "metadata": {},
   "source": [
    "## Step 3 — Install Mamba SSM CUDA Extensions\n",
    "\n",
    "> `causal-conv1d` **must be installed before** `mamba-ssm` because mamba-ssm depends on it at build time.  \n",
    "> `--no-build-isolation` lets pip reuse the already-installed torch/CUDA headers instead of rebuilding in an isolated env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bdfa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 of 2: causal-conv1d (dependency of mamba-ssm)\n",
    "!pip install causal-conv1d==1.4.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d72ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 of 2: mamba-ssm (CUDA kernel — this takes ~2-4 mins to compile)\n",
    "!pip install mamba-ssm==2.2.4 --no-build-isolation -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b916e9",
   "metadata": {},
   "source": [
    "## Step 4 — Install Remaining Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a4d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision / transformer stack (same versions as requirements.txt)\n",
    "!pip install \\\n",
    "    timm==1.0.15 \\\n",
    "    tensorboardX==2.6.2.2 \\\n",
    "    einops==0.8.1 \\\n",
    "    transformers==4.50.0 \\\n",
    "    Pillow==11.1.0 \\\n",
    "    requests==2.32.3 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solar irradiance / data science stack\n",
    "!pip install \\\n",
    "    pvlib==0.13.1 \\\n",
    "    pandas==2.3.3 \\\n",
    "    scikit-learn==1.7.2 \\\n",
    "    scipy==1.15.3 \\\n",
    "    matplotlib==3.10.8 \\\n",
    "    h5py==3.15.1 \\\n",
    "    PyYAML \\\n",
    "    tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: verify torch sees the GPU and mamba-ssm can be imported\n",
    "import torch\n",
    "print(f\"PyTorch  : {torch.__version__}\")\n",
    "print(f\"CUDA     : {torch.version.cuda}\")\n",
    "print(f\"GPU      : {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'NOT AVAILABLE'}\")\n",
    "\n",
    "import mamba_ssm\n",
    "print(f\"mamba-ssm: {mamba_ssm.__version__}  ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e28892",
   "metadata": {},
   "source": [
    "## Step 5 — Clone Repository & Install Local Package\n",
    "\n",
    "The `mambaVision` package is a **local, customized** version — it must be installed from your GitHub repo  \n",
    "(not the generic PyPI `mambavision`) so that `from mambaVision.models.mamba_vision import mamba_vision_B` resolves correctly.\n",
    "\n",
    "> Replace `YOUR_GITHUB_USERNAME/YOUR_REPO_NAME` with your actual GitHub repo URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42561d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ── CONFIGURATION ──────────────────────────────────────────────────────────────\n",
    "GITHUB_REPO_URL = \"https://github.com/YOUR_GITHUB_USERNAME/YOUR_REPO_NAME.git\"\n",
    "REPO_DIR        = \"/content/SolarMamba_repo\"   # Where the repo will be cloned\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Clone (or pull if already cloned — safe to re-run)\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {GITHUB_REPO_URL} {REPO_DIR}\n",
    "    print(f\"✅ Cloned to {REPO_DIR}\")\n",
    "else:\n",
    "    !git -C {REPO_DIR} pull\n",
    "    print(f\"✅ Repository already exists — pulled latest changes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the local mambaVision package in editable mode from the repo root.\n",
    "# This registers the `mambaVision` module so that\n",
    "# `from mambaVision.models.mamba_vision import mamba_vision_B` works inside Colab.\n",
    "%cd {REPO_DIR}\n",
    "!pip install -e . -q\n",
    "print(\"✅ Local mambaVision package installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d9166",
   "metadata": {},
   "source": [
    "## Step 6 — Load Dataset\n",
    "\n",
    "Choose **one** of the two options below. Run the cell that matches how your data is stored.\n",
    "\n",
    "| Option | Best for | Notes |\n",
    "|---|---|---|\n",
    "| **A — Google Drive** | Full Folsom ASI image archive | Mount Drive, point paths to your Drive folder |\n",
    "| **B — HuggingFace** | Versioned, shareable datasets | Requires HF account + uploaded dataset repo |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295414e8",
   "metadata": {},
   "source": [
    "### Option A — Google Drive Mount\n",
    "\n",
    "**Folder structure expected inside Drive:**\n",
    "```\n",
    "MyDrive/\n",
    "└── SolarMamba_Data/\n",
    "    ├── csv_files/\n",
    "    │   └── Folsom_irradiance_weather.csv\n",
    "    └── datasets/\n",
    "        └── 1_Folsom/\n",
    "            ├── 2014/\n",
    "            ├── 2015/\n",
    "            └── 2016/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d735a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Option A: Google Drive ─────────────────────────────────────────────────────\n",
    "# Run this cell if your dataset is stored in Google Drive.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify the expected paths exist\n",
    "import os\n",
    "DRIVE_DATA_ROOT = \"/content/drive/MyDrive/SolarMamba_Data\"\n",
    "CSV_PATH        = f\"{DRIVE_DATA_ROOT}/csv_files/Folsom_irradiance_weather.csv\"\n",
    "IMAGE_ROOT      = f\"{DRIVE_DATA_ROOT}/datasets/1_Folsom\"\n",
    "\n",
    "assert os.path.exists(CSV_PATH),   f\"❌ CSV not found at: {CSV_PATH}\"\n",
    "assert os.path.isdir(IMAGE_ROOT),  f\"❌ Image root not found at: {IMAGE_ROOT}\"\n",
    "print(f\"✅ CSV found     : {CSV_PATH}\")\n",
    "print(f\"✅ Image root    : {IMAGE_ROOT}\")\n",
    "print(f\"   Years present : {sorted(os.listdir(IMAGE_ROOT))}\")\n",
    "\n",
    "DATA_SOURCE = \"drive\"   # tag used in Step 7 to write config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5452a1a",
   "metadata": {},
   "source": [
    "### Option B — HuggingFace Dataset Download\n",
    "\n",
    "> Skip this cell if you chose Option A above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d2d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Option B: HuggingFace Dataset ─────────────────────────────────────────────\n",
    "# Run this cell INSTEAD of Option A if your data is on HuggingFace.\n",
    "# Requires: pip install huggingface_hub  (already included in transformers install above)\n",
    "\n",
    "# ── CONFIGURATION ──────────────────────────────────────────────────────────────\n",
    "HF_REPO_ID      = \"YOUR_HF_USERNAME/folsom-asi-dataset\"   # <-- Change this\n",
    "HF_REPO_TYPE    = \"dataset\"\n",
    "HF_LOCAL_DIR    = \"/content/data\"\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=HF_REPO_ID,\n",
    "    repo_type=HF_REPO_TYPE,\n",
    "    local_dir=HF_LOCAL_DIR,\n",
    "    ignore_patterns=[\"*.git*\", \"README*\"]\n",
    ")\n",
    "\n",
    "DRIVE_DATA_ROOT = HF_LOCAL_DIR\n",
    "CSV_PATH        = f\"{HF_LOCAL_DIR}/csv_files/Folsom_irradiance_weather.csv\"\n",
    "IMAGE_ROOT      = f\"{HF_LOCAL_DIR}/datasets/1_Folsom\"\n",
    "\n",
    "assert os.path.exists(CSV_PATH),  f\"❌ CSV not found at: {CSV_PATH}\"\n",
    "assert os.path.isdir(IMAGE_ROOT), f\"❌ Image root not found at: {IMAGE_ROOT}\"\n",
    "print(f\"✅ HuggingFace download complete.\")\n",
    "print(f\"   CSV: {CSV_PATH}\")\n",
    "print(f\"   Images: {IMAGE_ROOT}\")\n",
    "\n",
    "DATA_SOURCE = \"huggingface\"  # tag used in Step 7 to write config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad05dde",
   "metadata": {},
   "source": [
    "## Step 7 — Copy Pretrained Weights\n",
    "\n",
    "`mambavision_b_1k.pth` must be accessible at a known path.  \n",
    "The cell below looks for it in the GitHub repo first (e.g. stored via **Git LFS**),  \n",
    "and falls back to copying from your Drive if it's not in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b29ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "WEIGHT_FILENAME  = \"mambavision_b_1k.pth\"\n",
    "\n",
    "# This is where train.py will look for the weights (matches colab_pretrained_weights in config).\n",
    "# The cells below ensure the file ends up here regardless of source.\n",
    "FINAL_WEIGHT_PATH = f\"{REPO_DIR}/weights/{WEIGHT_FILENAME}\"\n",
    "os.makedirs(os.path.dirname(FINAL_WEIGHT_PATH), exist_ok=True)\n",
    "\n",
    "# ── Source priority ────────────────────────────────────────────────────────────\n",
    "# 1. Git LFS  — file is already in the cloned repo (best: no extra upload needed)\n",
    "# 2. Drive    — upload mambavision_b_1k.pth to MyDrive/ECCV_Irradiance/weights/\n",
    "#               and Drive is already mounted from Step 6A above.\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "REPO_WEIGHT_PATH  = FINAL_WEIGHT_PATH                                          # after git clone (LFS)\n",
    "DRIVE_WEIGHT_PATH = \"/content/drive/MyDrive/ECCV_Irradiance/weights/mambavision_b_1k.pth\"\n",
    "\n",
    "if os.path.exists(REPO_WEIGHT_PATH) and os.path.getsize(REPO_WEIGHT_PATH) > 1_000_000:\n",
    "    # Already present via Git LFS — nothing to do\n",
    "    WEIGHTS_PATH = REPO_WEIGHT_PATH\n",
    "    print(f\"✅ Weights already in repo (Git LFS): {WEIGHTS_PATH}\")\n",
    "\n",
    "elif os.path.exists(DRIVE_WEIGHT_PATH):\n",
    "    # Copy from Drive → repo weights dir so the path matches config exactly\n",
    "    shutil.copy2(DRIVE_WEIGHT_PATH, FINAL_WEIGHT_PATH)\n",
    "    WEIGHTS_PATH = FINAL_WEIGHT_PATH\n",
    "    print(f\"✅ Weights copied from Drive to: {WEIGHTS_PATH}\")\n",
    "\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Weights not found.\\n\"\n",
    "        f\"  Checked (Git LFS) : {REPO_WEIGHT_PATH}\\n\"\n",
    "        f\"  Checked (Drive)   : {DRIVE_WEIGHT_PATH}\\n\\n\"\n",
    "        f\"Fix: Upload 'mambavision_b_1k.pth' to your Drive at:\\n\"\n",
    "        f\"  MyDrive/ECCV_Irradiance/weights/mambavision_b_1k.pth\"\n",
    "    )\n",
    "\n",
    "print(f\"   File size : {os.path.getsize(WEIGHTS_PATH) / 1e6:.1f} MB\")\n",
    "print(f\"   Final path: {WEIGHTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf04eff",
   "metadata": {},
   "source": [
    "## Step 8 — Write `config_colab.yaml`\n",
    "\n",
    "This generates a clean config file for the Colab run, pointing to the  \n",
    "actual `/content/...` paths resolved in the cells above.  \n",
    "`env: \"colab\"` tells `data_loader.py` and `train.py` to use the `colab_*` keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, os\n",
    "\n",
    "# Paths resolved from the cells above\n",
    "config = {\n",
    "    \"env\": \"colab\",\n",
    "\n",
    "    \"data\": {\n",
    "        # --- kept for reference, not used in colab env ---\n",
    "        \"local_root\":  \"../mock_data_storage\",\n",
    "        \"server_root\": \"/storage2/CV_Irradiance\",\n",
    "        \"csv_path\":    \"/storage2/CV_Irradiance/datasets/1_Folsom/csv_files/Folsom_irradiance_weather.csv\",\n",
    "        \"image_root\":  \"/storage2/CV_Irradiance/datasets/1_Folsom\",\n",
    "\n",
    "        # --- ACTIVE colab paths ---\n",
    "        \"colab_root\":       DRIVE_DATA_ROOT,\n",
    "        \"colab_csv_path\":   CSV_PATH,\n",
    "        \"colab_image_root\": IMAGE_ROOT,\n",
    "\n",
    "        \"image_tolerance_sec\": 120,\n",
    "        \"dataset_type\":        \"folsom_colab_run\",\n",
    "        \"months\":              [],\n",
    "        \"data\": {\n",
    "            \"years\": [2014, 2015, 2016]\n",
    "        },\n",
    "\n",
    "        # Sampling\n",
    "        \"sampling_rate_sec\": 60,\n",
    "        \"sequence_length\":   40,\n",
    "\n",
    "        # Image\n",
    "        \"image_size\": 512,\n",
    "\n",
    "        # Loader — num_workers is auto-capped to 2 on Colab in data_loader.py\n",
    "        \"batch_size\":   8,    # Reduce if you hit CUDA OOM on T4 (15 GB VRAM)\n",
    "        \"num_workers\":  2,\n",
    "    },\n",
    "\n",
    "    \"model\": {\n",
    "        \"visual_backbone\":         \"mamba_vision_B\",\n",
    "        \"temporal_channels\":       7,\n",
    "        \"horizons\":                [1, 5, 10, 15],\n",
    "        \"pretrained_weights\":      WEIGHTS_PATH,\n",
    "        \"colab_pretrained_weights\": WEIGHTS_PATH,\n",
    "    },\n",
    "\n",
    "    \"training\": {\n",
    "        \"epochs\":        50,\n",
    "        \"learning_rate\": 5.0e-5,\n",
    "        \"weight_decay\":  0.1,\n",
    "        \"val_split\":     0.1,\n",
    "        \"test_split\":    0.1,\n",
    "        \"seed\":          42,\n",
    "    },\n",
    "}\n",
    "\n",
    "CONFIG_PATH = f\"{REPO_DIR}/SolarMamba/config_colab.yaml\"\n",
    "with open(CONFIG_PATH, \"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"✅ config_colab.yaml written to: {CONFIG_PATH}\")\n",
    "print(f\"   colab_root      : {config['data']['colab_root']}\")\n",
    "print(f\"   colab_csv_path  : {config['data']['colab_csv_path']}\")\n",
    "print(f\"   colab_image_root: {config['data']['colab_image_root']}\")\n",
    "print(f\"   weights         : {config['model']['pretrained_weights']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f27d8a",
   "metadata": {},
   "source": [
    "## Step 9 — Run Training\n",
    "\n",
    "Checkpoints are saved to `SolarMamba/Results/checkpoints/folsom_colab_run/` inside the repo directory.  \n",
    "To persist them across Colab sessions, copy the Results folder back to Drive after training (see the cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SOLAR_MAMBA_DIR = f\"{REPO_DIR}/SolarMamba\"\n",
    "os.chdir(SOLAR_MAMBA_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# sys.path must include repo root so that `from mambaVision.models...` resolves\n",
    "import sys\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "# Launch training — stdout/stderr are streamed directly into this cell's output\n",
    "!python train.py --config config_colab.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285af296",
   "metadata": {},
   "source": [
    "## Step 10 — (Optional) Back Up Checkpoints to Drive\n",
    "\n",
    "Colab VMs are **ephemeral** — all `/content/` data is lost when the session ends.  \n",
    "Run this cell periodically or after training to save checkpoints to Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f344a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "from datetime import datetime\n",
    "\n",
    "SRC_RESULTS  = f\"{REPO_DIR}/SolarMamba/Results\"\n",
    "BACKUP_DIR   = f\"/content/drive/MyDrive/SolarMamba_Checkpoints/{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "if os.path.exists(SRC_RESULTS):\n",
    "    shutil.copytree(SRC_RESULTS, BACKUP_DIR)\n",
    "    print(f\"✅ Checkpoints backed up to Drive: {BACKUP_DIR}\")\n",
    "    # List best models\n",
    "    for root, dirs, files in os.walk(BACKUP_DIR):\n",
    "        for f in files:\n",
    "            if \"best\" in f:\n",
    "                print(f\"   Best model: {os.path.join(root, f)}\")\n",
    "else:\n",
    "    print(f\"⚠️  No Results folder found at {SRC_RESULTS} — training may not have started yet.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
